{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.random import rand\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import reduce\n",
    "from functools import wraps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualizacoes import display_vec, display_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultiLayer Perceptron\n",
    "\n",
    "<img src='https://www.learnopencv.com/wp-content/uploads/2017/10/mlp-diagram.jpg' width='600px'>\n",
    "\n",
    "Podemos escrever a trasferência (somatória) do neurônio $k$ da camada $l$ de $z^{l}_{k}$ apenas adaptando um pouco a equação do perceptron simples adicionando a notação de índice das camadas de neuronios e substituindo $x$ por $a$ uma vez que o input de um neurônio da camada $l$ são as ativações $a$ dos neurônios da camada $l-1$:\n",
    "\n",
    "$$ \\normalsize z^{l}_{k} = \\sum_{i=1}^{m} w^{l}_i a^{l - 1}_i + b^{l}_k $$ \n",
    "\n",
    "E a ativação do neurônio $z^{l}_{k}$ é dado por:\n",
    "\n",
    "$$ \\normalsize a^{l}_{k} = \\sigma(z^{l}_{k}) $$\n",
    "\n",
    "\n",
    "Quanto à camada de saída de uma rede neural, temos algumas coisas a se pensar. Podemos ter duas situações: classificação __binária__ (duas classes, como 0 e 1) ou __multiclasses__ (como, 1,2,3 ou mesmo, \"gato\", \"cachorro\", \"pessoa\", etc). Dependendo do caso, podemos optar por apenas um neurônio de saída (que geralmente terá ativação sigmoide/logística ou step), ou, caso tenhamos mais do que duas classes, podemos trabalhar com a função __softmax__, que é uma adaptação vetorial da função sigmoide que comporta múltiplas classes.\n",
    "\n",
    "Supondo que ao invés de ter um neurônio de saída, tenhamos $K$ neurônios, um para cada uma das $k$ classes possíveis. Seja $\\vec{a}$ um vetor contendo a ativação dos $k$ neurônios de saída, podemos utilizar a função softmax da seguinte forma:\n",
    "\n",
    "(1) $$ \\normalsize \\vec{a} = \\begin{bmatrix}a_1\\\\ a_2\\\\ \\vdots\\\\ a_k\\end{bmatrix} $$\n",
    "\n",
    "a função softmax é dada por:\n",
    "\n",
    "(2) $$ \\normalsize \\text{softmax}(\\vec{a}) = \\frac{e^{a_i}}{\\sum_{j=1}^{k} e^{a_j}} $$\n",
    "\n",
    "Como resultado, teremos um vetor de 0 a 1 que pode ser interpretado como a __probabilidade__ para cada classe:\n",
    "\n",
    "(3) $$ \\normalsize \\text{softmax}(\\vec{a}) = \\begin{bmatrix}P(a_1|\\vec{a})\\\\ P(a_2|\\vec{a})\\\\ \\vdots\\\\ P(a_k|\\vec{a})\\end{bmatrix} $$\n",
    "\n",
    "Sendo assim, a classificação é dada pela classe com maior probabilidade:\n",
    "\n",
    "(4) $$ \\text{argmax}(\\text{softmax}(\\vec{a})) $$\n",
    "\n",
    "Vamos ver um exemplo com um vetor $\\vec{a}$  aleatório:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1.3, 2.2, 4.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<br>$\\vec{a} = \\begin{bmatrix}1.30\\\\2.20\\\\4.50\\end{bmatrix}$<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_vec(a, '\\\\vec{a}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(a):\n",
    "    somatoria = sum([e(a_i) for a_i in a])\n",
    "    return np.array([e(a_i)/somatoria for a_i in a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_vec = softmax(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<br>$\\text{softmax}(\\vec{a}) = \\begin{bmatrix}0.04\\\\0.09\\\\0.88\\end{bmatrix}$<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_vec(probs_vec, '\\\\text{softmax}(\\\\vec{a})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs_vec.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs_vec.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neuralnet_functions as funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    Rectified Linear Unit\\n    x: escalar, mas a função suporta vetores através do decorador vectorize_func\\n    '"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "funcs.ReLU.__doc__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "func = softmax\n",
    "print(func.__name__)\n",
    "print(func.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLayer():\n",
    "    \n",
    "    def __init__(self, input_shape, n_neurons, activation):\n",
    "        self.n_inputs = input_shape\n",
    "        self.n_outputs = n_neurons\n",
    "        self.weights = np.random.rand(input_shape, n_neurons)\n",
    "        self.bias = np.random.rand(n_neurons)\n",
    "        self.erro = None\n",
    "        self.z = None\n",
    "        self.a = None\n",
    "        self.func = activation\n",
    "        self.n_params = reduce((lambda x, y: x * y), self.weights.shape) + n_neurons\n",
    "    \n",
    "    def transfer(self, X):\n",
    "        self.z = np.dot(X, self.weights) + self.bias\n",
    "        self.a = self.func(self.z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construindo uma arquitetura de MLP\n",
    "\n",
    "A imagem abaixo mostra a arquitetura que vamos construir. Os índices $I$, $H1$, $H2$ e $O$ representam os layers da rede neural, enquanto que os termos $W$ representam as matrizes de pesos entre as camadas. Os termos $n$, $m$, $k$ representam os índices da contagem de neurônios por layer. Essas notações serão importantes para que possamos entender bem durante a construção da rede, quais valores representam exatamente quais termos dentro da arquitetura da rede neural.\n",
    "\n",
    "\n",
    "<img src='imgs/MLP_architeture_overall.png' width='500px'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feedforward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abaixo exibimos o primeiro vetor da matriz $X$ contendo as variáveis independentes. Como alimentaremos a rede neural com vetores em $R^4$, precisamos construir uma camada de entrada (Input layer) com 4 neurônios - um para cada variável independente $x_n$. Repare abaixo que a matriz $X$ está ordenada como `n_samples` x `n_features` , ou seja, cada registro é uma linha, cada coluna é uma variável. Um ponto importante de ser mencionado, é que essas ordenações afetam a maneira com que armazenamos os pesos da rede neural. Algumas implementações ordenam $X$ como `n_samples` x `n_features` e a matriz de pesos como `n_outputs`x `n_inputs`, e outras, ao contrário. Aqui nessa demonstração, iremos manter a matriz $X$ na ordenação atual, e armazenaremos os coeficientes de pesos como matrizes `n_inputs` x `n_outputs`. Por exemplo, considere a matriz de pesos $W^{2}$. Ela terá formato 5 x 2:\n",
    "\n",
    "$$ \\normalsize W^{2} = \\begin{bmatrix} w_{11} & w_{12}\\\\  w_{21} & w_{22} \\\\  w_{31} & w_{32} \\\\  w_{41} & w_{42}\\\\  w_{51} & w_{52}\\end{bmatrix} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A matriz $W^2$, que armazena os pesos entre as camadas $H1$ e $H2$ possuí dimensões $m$ x $k$. Como mencionado, estamos armazenando os pesos como `n_inputs` x `n_outputs`. Cada linha representa os pesos que saem de um espectivo neurônio $k$ da camada $H1$ e cada coluna representa as conexões que chegam para cada neurônio da camada $H2$ conforme a ilustração abaixo:\n",
    "\n",
    "<img src='imgs/MLP_architeture_W2.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<br>$X = \\begin{bmatrix}1&0&1&0.58\\end{bmatrix}$<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensões da matriz: (2001 x 4)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display_matrix(X.values, n_rows=1, label='X')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primeiro hidden Layer, com 8 neurônios e ativação sigmoide:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "hiddenLayer1 = LinearLayer(input_shape=4, n_neurons=5, activation=funcs.sigmoide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<br>$W^1 = \\begin{bmatrix}0.65&0.29&0.15&0.80&0.89\\\\0.73&0.09&0.65&0.41&0.04\\\\0.62&0.64&0.12&0.31&0.58\\\\0.77&0.66&0.32&0.14&0.20\\end{bmatrix}$<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensões da matriz: (4 x 5)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<br>$b^1 = \\begin{bmatrix}0.25\\\\0.12\\\\0.44\\\\0.27\\\\0.23\\end{bmatrix}$<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_matrix(hiddenLayer1.weights, label='W^1')\n",
    "display_vec(hiddenLayer1.bias, 'b^1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por exemplo, para o primeiro input $x_1$, a transferência desse input no primeiro neurônio será:\n",
    "\n",
    "$$ \\normalsize z^{1}_1 =  w_{11} x_{11} + w_{21} x_{12} +  w_{31} x_{13} +  w_{41} x_{14} + b_1 $$\n",
    "\n",
    "e a ativação deste neurônio:\n",
    "\n",
    "$$ \\normalsize a^{1}_1 = \\sigma(z^{1}_1)$$\n",
    "\n",
    "$$ \\normalsize a^{1}_1 = \\frac{1}{1 + e^{-(z^{1}_1)}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "hiddenLayer1.transfer(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transferência $z$ dos 8 neurônios "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<br>$z^1 = \\begin{bmatrix}1.97&1.43&0.90&1.46&1.83\\end{bmatrix}$<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensões da matriz: (2001 x 5)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display_matrix(hiddenLayer1.z, n_rows=1, label='z^1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ativação dos 8 neurônios para $x_1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<br>$a^1 = \\begin{bmatrix}0.88&0.81&0.71&0.81&0.86\\end{bmatrix}$<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensões da matriz: (2001 x 5)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display_matrix(hiddenLayer1.a, n_rows=1, label='a^1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criando uma segunda hidden layer com 4 neurônios:\n",
    "\n",
    "O input para cada neurônio deste layer será a quantidade de neurônios na camada anterior, e o formato da matriz de pesos que conectam ambos os layers é dado por: número de neurônios na camada $l-1$ x número de neurônios na camada $l$. No nosso caso, 8x4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "hiddenLayer2 = LinearLayer(5, 2, funcs.sigmoide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<br>$W^2 = \\begin{bmatrix}0.37&0.75\\\\0.01&0.80\\\\0.72&0.02\\\\0.72&0.19\\\\0.55&0.52\\end{bmatrix}$<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensões da matriz: (5 x 2)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<br>$b^2 = \\begin{bmatrix}0.03\\\\0.49\\end{bmatrix}$<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_matrix(hiddenLayer2.weights, label='W^2')\n",
    "display_vec(hiddenLayer2.bias, 'b^2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "hiddenLayer2.transfer(hiddenLayer1.a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ativação dos 4 neurônios para os 5 primeiros inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<br>$a^2 = \\begin{bmatrix}0.87&0.92\\\\0.87&0.92\\\\0.87&0.92\\\\0.87&0.92\\\\0.85&0.89\\end{bmatrix}$<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensões da matriz: (2001 x 2)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display_matrix(hiddenLayer2.a, n_rows=5, label='a^2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criando um output layer com um neurônio e ativação sigmoide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputLayer = LinearLayer(2, 1, funcs.sigmoide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<br>$W^L = \\begin{bmatrix}0.41\\\\0.87\\end{bmatrix}$<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensões da matriz: (2 x 1)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<br>$b^L = \\begin{bmatrix}0.29\\end{bmatrix}$<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_matrix(outputLayer.weights, label='W^L')\n",
    "display_vec(outputLayer.bias, label='b^L')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputLayer.transfer(hiddenLayer2.a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<br>$a^L = \\begin{bmatrix}0.81\\\\0.81\\\\0.81\\\\0.81\\\\0.81\\end{bmatrix}$<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensões da matriz: (2001 x 1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display_matrix(outputLayer.a, n_rows=5, label='a^L')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Backpropagation\n",
    " \n",
    " #### Sinal do erro\n",
    " \n",
    " Sinal de erro de um neurônio $j$ da camada $l$ é basicamente quantificar o quanto a função de custo variou (para mais ou para menos) conforme a transferência $z$ do neurônio aumentou ou diminuiu:\n",
    " \n",
    " $$ \\normalsize \\delta^{L}_{j} = \\frac{\\partial C}{\\partial z^{L}_{j}} $$\n",
    " \n",
    " Onde $z^{l}_{j}$ é a transferência do neurônio, e $C$ é a função de custo da rede neural. \n",
    " \n",
    " Acontece que quando a transferência $z$ de um neurônio varia, a ativação $a$ deste neurônio também varia, uma vez que a ativação do neurônio é dada por:\n",
    " \n",
    " $$ \\normalsize a^{L}_{j} = \\sigma(z^{L}_{j}) $$\n",
    " \n",
    " Sendo assim, a equação (1) é expandida através da lei da cadeia e se transforma em:\n",
    " \n",
    " $$ \\normalsize \\delta^{L}_{j} = \\frac{\\partial C}{\\partial a^{L}_{j}} \\frac{\\partial a^{L}_{j}}{\\partial z^{L}_{j}} $$\n",
    " \n",
    " $$ \\normalsize \\delta^{L}_{j} = \\frac{\\partial C}{\\partial a^{L}_{j}} \\sigma'(z^{L}_{j}) $$\n",
    " \n",
    " Essa equação pode ser interpretada como o quanto o erro varia dado a ativação do neurônio, e essa quantidade sendo multiplicada por quanto a ativação varia conforme a transferência do neurônio. Supondo que o erro $C$ cresça bastante para a ativação $a^{l}_{j}$, isso nos daria uma quantidade positiva para o primeiro termo. Acontece, que precisamos ponderar essa variação levando em consideração o quanto a ativação do neurônio altera para o valor de transferência $z$, e é por isso que o termo acima é multiplicado pelo segundo termo.\n",
    "\n",
    "Supondo que a função de erro $C$ para o vetor de ativações $A$ e o vetor de valores resposta $Y$, seja o custo de erro quadrático, dado pela fórmula:\n",
    "\n",
    "$$ \\normalsize C(w,b) = \\frac{1}{2n} \\sum_{j=0}^{J} \\parallel y - a^{L} \\parallel^2 $$\n",
    "\n",
    "Onde, os argumentos desta função são justamente os pesos e bias da rede neural $W$ e $b$, que levarão ao vetor de ativações $a$ na camada de saída, que serão comparados com os valores originais $y$.\n",
    "\n",
    "Sendo assim, para um único $x_k$, o custo da sua previsão dado a ativação dos neurônios será:\n",
    "\n",
    "$$ \\normalsize C(w,b) = \\frac{1}{2} \\parallel y - a^{L} \\parallel^2$$  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = lambda y,a: (y - a)**2 / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.004049999999999997"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C(1, 0.91)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.008099999999999994"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1 - 0.91)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<br>$Y = \\begin{bmatrix}0\\\\0\\\\0\\\\0\\\\0\\end{bmatrix}$<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_vec(Y[:5], label='Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "custo_por_input = np.array([C(y, a) for y, a in zip(Y, outputLayer.a)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplicando a função de custo ás saídas da rede neural e suas respectivas respostas esperadas (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<br>$C(w,b) = \\begin{bmatrix}0.33\\\\0.33\\\\0.33\\\\0.33\\\\0.32\\end{bmatrix}$<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensões da matriz: (2001 x 1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display_matrix(custo_por_input, n_rows=5, label='C(w,b)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Já a __variação__ do custo em respeito a ativação do neurônio pode ser encontrada usando a regra da potência em derivação:\n",
    "\n",
    "$$ \\normalsize f(x) = x^n \\,\\, \\therefore \\,\\, \\frac{d}{dx} x^n = nx^{n-1}$$\n",
    "\n",
    "Sendo assim, voltando para o cálculo do gradiente de erro no neurônio de saída, temos então que o primeiro termo da equação pode ser computado como:\n",
    "\n",
    "$$ \\normalsize \\frac{\\partial C}{\\partial a^{l}_{j}} = \\frac{2}{2}(y_j - a^{L}_j)^{2-1}$$\n",
    "\n",
    "$$ \\normalsize \\frac{\\partial C}{\\partial a^{l}_{j}} = (y_j - a^{L}_j)$$\n",
    "\n",
    "Já para o segundo termo na equação do gradiente da camada de saída, precisamos encontrar a taxa de variação da função de ativação com respeito a transferência do neurônio (representado por $z$). Supondo que usamos a função sigmoide:\n",
    "\n",
    "$$ \\normalsize \\sigma(z) = \\frac{1}{1 + e^{-z}} $$\n",
    "\n",
    "Sua função derivada será:\n",
    "\n",
    "$$ \\normalsize \\sigma'(z) = \\sigma(z)(1 - \\sigma(z)) $$\n",
    "\n",
    "Portanto, o erro $\\delta^{L}_{j}$ para uma rede com função de ativação sigmoide e função de custo quadrática é dado por:\n",
    "\n",
    "$$ \\normalsize \\delta^{L} = (a^{L} - y) \\odot \\sigma(z)(1 - \\sigma(z)) $$\n",
    "\n",
    "Essa equação, sendo generalizada para outras funções de custo e ativação, transforma-se na equação abaixo, que é bastante vista em documentações, papers e artigos sobre o algoritmo backpropagation:\n",
    "\n",
    "$$ \\normalsize \\delta^{L} = \\nabla_a C \\odot \\sigma'(z) $$\n",
    "\n",
    "Onde $\\nabla_a C$ costuma representar um vetor contendo as derivadas parciais $ \\frac{\\partial C}{\\partial a^{l}_{j}}$, $\\odot$ representa uma operação de multiplicação <em>element-wise</em> entre vetores e $\\sigma'(z)$ representa a função derivada da função de ativação. No nosso caso, temos dois neurônios na camada de saída, ambos com ativação sigmoide. O vetor de erros da camada de output será então:\n",
    "\n",
    "(1) $$ \\normalsize \\delta^{L} = \\begin{bmatrix}(a^{L}_1 - y_1)\\\\(a^{L}_2 - y_2)\\end{bmatrix} \\odot  \\begin{bmatrix}\\sigma'(z^{L}_1)\\\\\\sigma'(z^{L}_2)\\end{bmatrix}$$\n",
    "\n",
    "(2) $$ \\normalsize \\delta^{L} = \\begin{bmatrix}(a^{L}_1 - y_1)\\,\\sigma'(z^{L}_1)\\\\(a^{L}_2 - y_2)\\,\\sigma'(z^{L}_2)\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "derivada_sigmoide = lambda z: funcs.sigmoide(z)*(1 - funcs.sigmoide(z))\n",
    "derivadas_parciais =  lambda a, y: a - y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "nabla_a_C = np.array([derivadas_parciais(a, y) for a, y in zip(outputLayer.a, Y)]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<br>$\\nabla_a C = \\begin{bmatrix}0.81\\end{bmatrix}$<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensões da matriz: (1 x 2001)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display_matrix(nabla_a_C.T, n_rows=2, n_cols=1, label='\\\\nabla_a C')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "deriv_sigma_z = np.array([derivada_sigmoide(z) for z in outputLayer.z])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<br>$\\sigma'(z) = \\begin{bmatrix}0.15\\end{bmatrix}$<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensões da matriz: (1 x 2001)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display_matrix(deriv_sigma_z.T, n_rows=2, n_cols=1, label=\"\\sigma'(z)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputLayer.error = (nabla_a_C * deriv_sigma_z).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2001)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputLayer.error.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<br>$\\delta^{L} = \\begin{bmatrix}0.12\\end{bmatrix}$<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensões da matriz: (1 x 2001)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display_matrix(outputLayer.error, n_rows=2, n_cols=1, label='\\\\delta^{L}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradiente da função de erro em respeito aos pesos\n",
    "\n",
    "O objetivo do algoritmo de backpropagation, como mencionado, é encontrar a combinação de pesos da rede neural que em conjunto retornam o menor erro. O gradiente da função de custo em relação ao peso $w_{ij}$ da camada $l$ da rede neural é dado por:\n",
    "\n",
    "$$ \\normalsize \\frac{\\partial C}{\\partial w^{L}_{ij}} $$\n",
    "\n",
    "Através da __regra da cadeia__, podemos expandir para:\n",
    "\n",
    "\n",
    "$$ \\normalsize \\frac{\\partial C}{\\partial w^{L}_{ij}} = \\frac{\\partial C}{\\partial z^{L}_{j}} \\frac{\\partial z^{L}_{j}}{\\partial w^{L}_{ij}}$$\n",
    "\n",
    "Já sabemos que $\\frac{\\partial C}{\\partial z^{L}_{j}}$ é o sinal do erro:\n",
    "\n",
    " $$ \\normalsize \\delta^{L}_{j} = \\frac{\\partial C}{\\partial z^{L}_{j}} $$\n",
    " \n",
    " Já a parcial $\\frac{\\partial z^{L}_{j}}{\\partial w^{L}_{ij}}$ é a ativação do neurônio de entrada da camada:\n",
    " \n",
    " $$ \\normalsize \\frac{\\partial z^{L}_{j}}{\\partial w^{L}_{ij}} = a^{l-1}_i $$\n",
    " \n",
    " Finalmente, podemos reescrever a equação do gradiente do custo em relação ao peso $w_{ij}$ como:\n",
    " \n",
    " $$ \\normalsize \\frac{\\partial C}{\\partial w^{L}_{ij}} = \\delta^{L}_{j}\\,a^{l-1}_i  $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<br>$a^2 = \\begin{bmatrix}0.87&0.92\\\\0.87&0.92\\\\0.87&0.92\\\\0.87&0.92\\\\0.85&0.89\\end{bmatrix}$<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensões da matriz: (2001 x 2)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display_matrix(hiddenLayer2.a, n_rows=5, label='a^2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradientes_output = np.dot(hiddenLayer2.a.T, outputLayer.error.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<br>$\\begin{bmatrix}81.97\\\\86.38\\end{bmatrix}$<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensões da matriz: (2 x 1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display_matrix(gradientes_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Propagando o erro nas camadas ocultas\n",
    "\n",
    "Conseguimos chegar ao vetor de gradientes da camada de saída. \n",
    "\n",
    "Supondo que a última camada oculta da nossa rede seja a camada $l$. O que faremos é multiplicar o erro calculado nos neurônios da camada de saída $L$ pelos pesos que conectam a camada $l$ a camada $L$ (volte na __visualização da matriz de pesos__ da camada de output), que como vimos, é uma matriz 4x2. Cada um desses 4 valores serão então multiplicados pela função derivada da sigmoide para cada um dos valores dos neurônios da camada $l$. O erro portanto, do neurônio $j$ da camada $l$ é dado por: \n",
    "\n",
    "$$ \\Large \\delta^{l}_j = (\\sum_{j=1}^{k} w^{l + 1}_j \\delta^{l+1}_j)\\, \\sigma'(z^l_j) $$\n",
    "\n",
    "Onde $w^{l + 1}$ é a matriz de pesos da camada $l + 1$ e $\\delta^{l+1}$ é o vetor de sinais de erro da camada $l + 1$. Levando em consideração que armazenamos o erro dos neurônios de saída em um vetor, e os pesos estão armazenados em uma matriz $W$, podemos reescrever a equação acima como:\n",
    "\n",
    "$$ \\Large \\delta^{l} = W^{l + 1} \\delta^{l+1} \\odot \\sigma'(z^l) $$\n",
    "\n",
    "Voltando ao nosso caso, queremos calcular os gradientes de erro da penultima camada da rede, a `hiddenLayer2`. Precisamos calcular então ambos os termos da equação acima novamente. O primeiro é a multiplicação da matriz de pesos da camada de saída $L$ pelo vetor de erros dos dois neurônios da camada $L$, e finalmente multiplicar esses valores pelas respectivas derivadas das ativações dos neurônios da `hiddenLayer2`, representadas aqui como a camada $l$:\n",
    "\n",
    "$$ \\normalsize \\delta^{l} = \\begin{bmatrix} W^L_1\\delta^{L} \\\\ W^L_2\\delta^{L}\\\\ W^L_3\\delta^{L}\\\\ W^L_4\\delta^{L}\\end{bmatrix} \\odot  \\begin{bmatrix}\\sigma'(z^{l}_1)\\\\\\sigma'(z^{l}_2)\\\\ \\sigma'(z^{l}_3)\\\\\\sigma'(z^{l}_4)\\end{bmatrix} $$\n",
    "\n",
    "Com base nisso, vamos calcular o gradiente de erro para o `hiddenLayer2`. Primeiro calculamos o vetor contendo $W^L \\delta^L$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_L_Delta = np.dot(outputLayer.weights, outputLayer.error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<br>$W^L \\delta^L = \\begin{bmatrix}0.05\\\\0.11\\end{bmatrix}$<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensões da matriz: (2 x 2001)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display_matrix(W_L_Delta, n_cols=1, label='W^L \\delta^L')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "deriv_hiddenlayer2 = np.array([derivada_sigmoide(z) for z in hiddenLayer2.z])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<br>$\\sigma'(z) = \\begin{bmatrix}0.11\\\\0.08\\end{bmatrix}$<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensões da matriz: (2 x 2001)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display_matrix(deriv_hiddenlayer2.T, n_cols=1, label=\"\\sigma'(z)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "hiddenLayer2.error = W_L_Delta * deriv_hiddenlayer2.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<br>$\\delta^{l} = \\begin{bmatrix}0.01\\\\0.01\\end{bmatrix}$<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensões da matriz: (2 x 2001)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display_matrix(hiddenLayer2.error, n_cols=1, label='\\\\delta^{l}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Propagando o erro para a primeira camada oculta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_L_Delta_1 = np.dot(hiddenLayer2.weights, hiddenLayer2.error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<br>$W^L \\delta^L = \\begin{bmatrix}0.01\\\\0.01\\\\0.00\\\\0.01\\\\0.01\\end{bmatrix}$<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensões da matriz: (5 x 2001)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display_matrix(W_L_Delta_1, n_cols=1, label='W^L \\delta^L')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "deriv_hiddenlayer1 = np.array([derivada_sigmoide(z) for z in hiddenLayer1.z])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<br>$\\sigma'(z) = \\begin{bmatrix}0.11\\\\0.16\\\\0.21\\\\0.15\\\\0.12\\end{bmatrix}$<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensões da matriz: (5 x 2001)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display_matrix(deriv_hiddenlayer1.T, n_cols=1, label=\"\\sigma'(z)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "hiddenLayer1.error = W_L_Delta_1 * deriv_hiddenlayer1.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<br>$\\delta^{l} = \\begin{bmatrix}0.00\\\\0.00\\\\0.00\\\\0.00\\\\0.00\\end{bmatrix}$<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensões da matriz: (5 x 2001)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display_matrix(hiddenLayer1.error, n_cols=1, label='\\\\delta^{l}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradiente da função de custo em relação aos pesos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Codificando um MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLayer():\n",
    "    \n",
    "    def __init__(self, input_shape, n_neurons, activation):\n",
    "        self.n_inputs = input_shape\n",
    "        self.n_outputs = n_neurons\n",
    "        self.weights = np.random.rand(input_shape, n_neurons)\n",
    "        self.bias = np.random.rand(n_neurons)\n",
    "        self.erro = np.zeros(n_neurons)\n",
    "        self.z = None\n",
    "        self.a = None\n",
    "        self.func = activation\n",
    "        self.n_params = reduce((lambda x, y: x * y), self.weights.shape) + n_neurons\n",
    "    \n",
    "    def transfer(self, X):\n",
    "        self.z = np.dot(X, self.weights) + self.bias\n",
    "        self.a = self.func(self.z)\n",
    "\n",
    "\n",
    "class MultiLayerPerceptron():\n",
    "    \n",
    "    def __init__(self, n_inputs):\n",
    "        self.n_inputs = n_inputs\n",
    "        self.network = []\n",
    "        self.trainable_params = 0\n",
    "        self.n_layers = 0\n",
    "        self.n_neurons = 0\n",
    "        self.coefs = None\n",
    "        self.intercepts = None\n",
    "            \n",
    "        \n",
    "    def info(self):\n",
    "        print('Número de neurônios: %d' % self.n_neurons)\n",
    "        print('Número de layers: %d' % self.n_layers)\n",
    "        print('Número de parâmetros treináveis: %d' % self.trainable_params)\n",
    "        print('\\nExibindo estrutura dos layers: \\n')\n",
    "        for i, vals in enumerate(self.network):\n",
    "            print('Layer %d:' % i, '{}, '.format(vals.weights.shape), ' Parâmetros: %d' % self.network[i].n_params)\n",
    "\n",
    "            \n",
    "    def update_coefs(self, coefs, bias):\n",
    "        for l in range(self.n_layers):\n",
    "            self.network[l].weights = coefs[l]\n",
    "            self.network[l].bias = bias[l]\n",
    "            \n",
    "        self.coefs = np.array([self.network[i].weights for i in range(self.n_neurons)])\n",
    "        self.intercepts = np.array([self.network[i].bias for i in range(self.n_neurons)])            \n",
    "            \n",
    "            \n",
    "    def add_layer(self, n_neurons, activation):\n",
    "        if len(self.network) == 0:\n",
    "            # primeiro layer da rede\n",
    "            layer_obj = LinearLayer(self.n_inputs, n_neurons, activation)\n",
    "        else:\n",
    "            layer_input = self.network[-1].n_outputs\n",
    "            layer_obj = LinearLayer(layer_input, n_neurons, activation)\n",
    "            \n",
    "        self.network.append(layer_obj)\n",
    "        self.trainable_params += layer_obj.n_params\n",
    "        self.n_layers = len(self.network)\n",
    "        self.n_neurons += 1\n",
    "        self.coefs = np.array([self.network[i].weights for i in range(self.n_neurons)])\n",
    "        self.intercepts = np.array([self.network[i].bias for i in range(self.n_neurons)])\n",
    "    \n",
    "    \n",
    "    def make_decision_function(self):\n",
    "        thresholds = {'sigmoide':.5, 'tahn':0, 'sign':1}\n",
    "        output_activation = self.network[-1].func.__name__\n",
    "        \n",
    "        if output_activation in thresholds.keys():\n",
    "            return lambda a: 1 if a >= thresholds[output_activation] else 0\n",
    "        \n",
    "        elif output_activation == 'linear':\n",
    "            return lambda a: a\n",
    "        \n",
    "        elif output_activation == 'softmax':\n",
    "            return np.argmax\n",
    "        \n",
    "        else:\n",
    "            raise 'Ativação da camada de saída não reconhecida'        \n",
    "        \n",
    "\n",
    "    def predict(self, X):\n",
    "        for l in range(len(self.network)):\n",
    "            print(self.network[l].weights)\n",
    "            if l == 0:\n",
    "                # se for input\n",
    "                self.network[l].transfer(X)\n",
    "            else:\n",
    "                # se for output\n",
    "                self.network[l].transfer(self.network[l - 1].a)\n",
    "                \n",
    "        A = self.network[-1].a\n",
    "        decision_function = self.make_decision_function()\n",
    "        return list(map(decision_function, A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(n_inputs, hiddenLayers=(6,), activation=funcs.sigmoide, n_classes=2, output_activation=funcs.sigmoide):\n",
    "    # Instancia o modelo\n",
    "    modelo = MultiLayerPerceptron(n_inputs = n_inputs)\n",
    "    # constrói as camadas ocultas\n",
    "    for layer in hiddenLayers:\n",
    "        modelo.add_layer(layer, activation)\n",
    "    # define camada de output\n",
    "    if n_classes == 2:\n",
    "        modelo.add_layer(1, output_activation)\n",
    "    else:\n",
    "        modelo.add_layer(n_classes, output_activation)\n",
    "    # retorna objeto\n",
    "    return modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP = MultiLayerPerceptron(n_inputs=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP.add_layer(4, funcs.sigmoide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP.add_layer(1, funcs.sigmoide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "previsoes = MLP.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(Y, previsoes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(4), activation='logistic', solver='sgd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(mlp.coefs_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.intercepts_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(Y, mlp.predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_coefs = np.array(mlp.coefs_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_bias = np.array(mlp.intercepts_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP.update_coefs(new_coefs, new_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP.coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "previsoes = MLP.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(Y, previsoes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative(func, x, dx=1e-6):\n",
    "    return (func(x + dx) - func(x)) / dx\n",
    "\n",
    "def funcaoDerivada(X, func, dx=1e-6):\n",
    "    return [derivative(func, x, dx) for x in X]\n",
    "\n",
    "def plotFuncDeriv(X, func):\n",
    "    plt.plot(X, funcaoDerivada(X, func))\n",
    "    plt.plot(X, list(map(func,X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotFuncDeriv(np.linspace(-10,10,100), funcs.sigmoide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotFuncDeriv(np.linspace(-10,10,100), funcs.tanh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotFuncDeriv(np.linspace(-10,10,100), funcs.ReLU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def QuadraticCost(Y, A):\n",
    "    diff = (Y - A)**2\n",
    "    return diff/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = build_model(n_inputs=6, hiddenLayers=(4,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = clf.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_vec(A[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_vec(Y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "erro_output = QuadraticCost(Y, A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "erro_output[:5]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
